{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd473a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(176.38325239532747, 1343864)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open('dataforchunk/lumberchunker.json', 'r', encoding='utf-8') as file:  \n",
    "    qa_data = json.load(file)\n",
    "len_sents=0\n",
    "len_lists=0\n",
    "for sentence in qa_data:\n",
    "    len_sents+=len(sentence)\n",
    "    len_lists+=1\n",
    "len_sents/len_lists,len_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cbd402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.656591183537294 4390400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(178.53288892458437, 4445826)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dynamically merge segmented files\n",
    "import json  \n",
    "import os\n",
    "target_size=211\n",
    "filename='multifieldqa_zh/qwen25_14B_set.json' \n",
    "chunk_path='dataforchunk/multifieldqa_zh/qwen25_14B_set.json'    \n",
    "\n",
    "all_qa_data=[]\n",
    "with open(chunk_path, 'r', encoding='utf-8') as cfile: \n",
    "    qa_data = json.load(cfile)\n",
    "\n",
    "for item in qa_data: \n",
    "    all_qa_data=all_qa_data+item[\"gpt_output\"]  # \"final_chunks\"  \"gpt_output\"\n",
    "# all_qa_data=qa_data\n",
    "\n",
    "len_sents=0\n",
    "len_lists=0\n",
    "for sentence in all_qa_data:\n",
    "    len_sents+=len(sentence)\n",
    "    len_lists+=1\n",
    "print(len_sents/len_lists,len_sents)\n",
    "\n",
    "merged_paragraphs = []  \n",
    "current_paragraph = \"\" \n",
    "for i,paragraph in enumerate(all_qa_data):  \n",
    "    if not isinstance(paragraph, str):\n",
    "        if isinstance(paragraph, list):\n",
    "            print(i,paragraph)\n",
    "            paragraph = ' '.join(str(item) for item in paragraph)\n",
    "    if len(current_paragraph) + len(paragraph) <= target_size:  \n",
    "        # try:\n",
    "        current_paragraph +=' '+paragraph  \n",
    "        # except:\n",
    "        #     print(i,paragraph)\n",
    "    else:  \n",
    "        merged_paragraphs.append(current_paragraph)  \n",
    "        current_paragraph = paragraph  \n",
    "if current_paragraph:  \n",
    "    merged_paragraphs.append(current_paragraph)  \n",
    "\n",
    "with open(filename, 'w', encoding='utf-8') as sfile:\n",
    "    json.dump(merged_paragraphs, sfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(filename, 'r', encoding='utf-8') as file:  \n",
    "    qa_data = json.load(file)\n",
    "len_sents=0\n",
    "len_lists=0\n",
    "for sentence in qa_data:\n",
    "    len_sents+=len(sentence)\n",
    "    len_lists+=1\n",
    "len_sents/len_lists,len_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5167cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126.18601969307194 1422495\n",
      "178.61564540431945 1422495\n"
     ]
    }
   ],
   "source": [
    "# Merge the chunk results of each document according to the specified length\n",
    "def merge_short_texts(chunks, min_length=30, max_length=200):\n",
    "    new_chunks = []\n",
    "    for i,text in enumerate(chunks):\n",
    "        if len(text) < min_length:\n",
    "            if i==0:\n",
    "                new_chunks.append(text)\n",
    "            else:\n",
    "                if len(new_chunks[-1]) + len(text) <= max_length:\n",
    "                    new_chunks[-1] += text\n",
    "                else:\n",
    "                    new_chunks.append(text)\n",
    "        else:\n",
    "            new_chunks.append(text)\n",
    "    return new_chunks\n",
    "\n",
    "import json  \n",
    "import os\n",
    "root_dir = 'multifieldqa_zh/chunk_1.5all3_M_merge.json'\n",
    "filename = 'dataforchunk/multifieldqa_zh/chunk_1.5all3_M.json'\n",
    " \n",
    "min_length = 80\n",
    "max_length = 225\n",
    "\n",
    "with open(filename, 'r', encoding='utf-8') as file:  \n",
    "    qa_data = json.load(file)\n",
    "len_sents=0\n",
    "len_lists=0\n",
    "for item in qa_data:\n",
    "    for sentence in item[\"final_chunks\"]:\n",
    "        len_sents+=len(sentence)\n",
    "        len_lists+=1\n",
    "print(len_sents/len_lists,len_sents)\n",
    "\n",
    "all_data_chunk=[]\n",
    "for item in qa_data: \n",
    "    all_data_chunk += merge_short_texts(item[\"final_chunks\"], min_length, max_length)\n",
    "    \n",
    "with open(root_dir, 'w', encoding='utf-8') as sfile:\n",
    "    json.dump(all_data_chunk, sfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "len_sents=0\n",
    "len_lists=0\n",
    "for sentence in all_data_chunk:\n",
    "    len_sents+=len(sentence)\n",
    "    len_lists+=1\n",
    "print(len_sents/len_lists,len_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cea4908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the chunk results of each document according to the specified length\n",
    "def merge_short_texts(item, min_length=30, max_length=200):\n",
    "    new_chunks = []\n",
    "    new_outline = []\n",
    "    new_core = []\n",
    "    for i,text in enumerate(item[\"final_chunk\"]):\n",
    "        if len(text) < min_length:\n",
    "            if i==0:\n",
    "                new_chunks.append(text)\n",
    "                new_outline.append(item[\"outline\"][i])\n",
    "                new_core.append(item[\"core_content\"][i])\n",
    "            else:\n",
    "                if len(new_chunks[-1]) + len(text) <= max_length:\n",
    "                    new_chunks[-1] += text\n",
    "                else:\n",
    "                    new_chunks.append(text)\n",
    "                    new_outline.append(item[\"outline\"][i])\n",
    "                    new_core.append(item[\"core_content\"][i])\n",
    "        else:\n",
    "            new_chunks.append(text)\n",
    "            new_outline.append(item[\"outline\"][i])\n",
    "            new_core.append(item[\"core_content\"][i])\n",
    "    save = {\"outline\":new_outline,\"core_content\":new_core,\"final_chunk\":new_chunks}\n",
    "    return save\n",
    "\n",
    "import json  \n",
    "import os\n",
    "root_dir = 'multifieldqa_zh/ratio_7B.json'\n",
    "filename = 'dataforchunk/multifieldqa_zh/ratio_7B.json'\n",
    "\n",
    "min_length = 50\n",
    "max_length = 250\n",
    "\n",
    "with open(filename, 'r', encoding='utf-8') as file:  \n",
    "    qa_data = json.load(file)\n",
    "len_sents=0\n",
    "len_lists=0\n",
    "for item in qa_data:\n",
    "    for sentence in item[\"final_chunk\"]:\n",
    "        len_sents+=len(sentence)\n",
    "        len_lists+=1\n",
    "print(len_sents/len_lists,len_sents)\n",
    "\n",
    "all_data_chunk=[]\n",
    "for item in qa_data: \n",
    "    all_data_chunk.append(merge_short_texts(item, min_length, max_length))\n",
    "    \n",
    "with open(root_dir, 'w', encoding='utf-8') as sfile:\n",
    "    json.dump(all_data_chunk, sfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "len_sents=0\n",
    "len_lists=0\n",
    "for item in all_data_chunk:\n",
    "    for sentence in item[\"final_chunk\"]:\n",
    "        len_sents+=len(sentence)\n",
    "        len_lists+=1\n",
    "print(len_sents/len_lists,len_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48017030",
   "metadata": {},
   "source": [
    "## 几种不同分块方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea5e03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'multifieldqa_zh/ratio_7B.json'\n",
    "filename = 'dataforchunk/multifieldqa_zh/ratio_7B.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbb03a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5722924f7924771ae6ee62cee3b0e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178.34745968722146 4401972\n"
     ]
    }
   ],
   "source": [
    "from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "root_dir = 'raw/multifieldqa_zh'\n",
    "all_data=[]\n",
    "for root, dirs, files in os.walk(root_dir):  \n",
    "    for file in files:\n",
    "        documents = SimpleDirectoryReader(input_files=[os.path.join(root, file)]).load_data() \n",
    "        node_parser = SimpleNodeParser.from_defaults(\n",
    "            chunk_size=281, chunk_overlap=0)\n",
    "        nodes_tmp = node_parser.get_nodes_from_documents(documents, show_progress=True)\n",
    "\n",
    "        nodes=[]\n",
    "        for node in nodes_tmp:\n",
    "            i=node.text\n",
    "            nodes.append(i)\n",
    "        all_data+=nodes  \n",
    "with open('dataforchunk/multifieldqa_zh/llamaindex.json', 'w', encoding='utf-8') as sfile:\n",
    "    json.dump(all_data, sfile, ensure_ascii=False, indent=4)\n",
    "len_sents=0\n",
    "len_lists=0\n",
    "for sentence in all_data:\n",
    "    len_sents+=len(sentence)\n",
    "    len_lists+=1\n",
    "print(len_sents/len_lists,len_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a14148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178.99882142566852 4404445\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "chunk_size=179\n",
    "with open('raw/multifieldqa_zh/1.txt', 'r', encoding='utf-8') as file:  \n",
    "    # 读取文件内容  \n",
    "    content = file.read()  \n",
    "# 打印文件内容  \n",
    "chunks = [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)] \n",
    "            \n",
    "with open('dataforchunk/multifieldqa_zh/chunk_original.json', 'w', encoding='utf-8') as sfile:\n",
    "    json.dump(chunks, sfile, ensure_ascii=False, indent=4)\n",
    "len_sents=0\n",
    "len_lists=0\n",
    "for sentence in chunks:\n",
    "    len_sents+=len(sentence)\n",
    "    len_lists+=1\n",
    "print(len_sents/len_lists,len_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa36d031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/u2023000898/envs/chunk/lib/python3.10/site-packages/llama_index/download/module.py:12: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c688fb87d4d34bec919cfdb649b73f46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.868 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9013fd74b040f19690cfcb06f6f26e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/98288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "程序执行时间为: 317.780415058136 秒\n",
      "178.80218125584992 4393706\n"
     ]
    }
   ],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "import json\n",
    "import time\n",
    "\n",
    "start_time = time.time() \n",
    "save_path='dataforchunk/multifieldqa_zh/semantic_75.json'\n",
    "split_path='raw/multifieldqa_zh'\n",
    "documents = SimpleDirectoryReader(split_path).load_data()\n",
    "\n",
    "embed_model=HuggingFaceEmbedding(model_name=\"BAAI/bge-base-zh-v1.5\")  \n",
    "splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1, breakpoint_percentile_threshold=75, embed_model=embed_model   \n",
    ")                   # Chinese needs to modify the sentence segmentation function: split_by_my_zh\n",
    "\n",
    "all_data=[]\n",
    "for item in documents:\n",
    "    nodes = splitter.get_nodes_from_documents([item], show_progress=True)\n",
    "\n",
    "    for node in nodes:\n",
    "        if node.text.strip() !='':\n",
    "            all_data.append(node.text)\n",
    "        \n",
    "with open(save_path, 'w', encoding='utf-8') as sfile:\n",
    "    json.dump(all_data, sfile, ensure_ascii=False, indent=4)\n",
    "end_time = time.time()  \n",
    "\n",
    "execution_time = end_time - start_time  \n",
    "print(f\"程序执行时间为: {execution_time} 秒\")\n",
    "\n",
    "len_sents=0\n",
    "len_lists=0\n",
    "for sentence in all_data:\n",
    "    len_sents+=len(sentence)\n",
    "    len_lists+=1\n",
    "print(len_sents/len_lists,len_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca63a47",
   "metadata": {},
   "source": [
    "## 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fd2033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import evaluate\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import jieba\n",
    "def bleu_score(\n",
    "    continuation: str,\n",
    "    reference: str,\n",
    "    with_penalty = False\n",
    ") -> float:\n",
    "    f = lambda text: list(jieba.cut(text))\n",
    "    bleu = evaluate.load('src/.cache/huggingface/bleu')\n",
    "    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n",
    "    \n",
    "    bleu_avg = results['bleu']\n",
    "    bleu1 = results['precisions'][0]\n",
    "    bleu2 = results['precisions'][1]\n",
    "    bleu3 = results['precisions'][2]\n",
    "    bleu4 = results['precisions'][3]\n",
    "    brevity_penalty = results['brevity_penalty']\n",
    "\n",
    "    if with_penalty:\n",
    "        return bleu_avg, bleu1, bleu2, bleu3, bleu4\n",
    "    else:\n",
    "        return 0.0 if brevity_penalty==0 else bleu_avg/brevity_penalty, bleu1, bleu2, bleu3, bleu4\n",
    "\n",
    "def rougeL_score(\n",
    "    continuation: str,\n",
    "    reference: str\n",
    ") -> float:\n",
    "    f = lambda text: list(jieba.cut(text))\n",
    "    rouge = evaluate.load('src/.cache/huggingface/rouge')\n",
    "    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f, rouge_types=['rougeL'])\n",
    "    score = results['rougeL']\n",
    "    return score\n",
    "\n",
    "folder_path = 'dataforchunk/ccc_eval'\n",
    "for filename in os.listdir(folder_path):\n",
    "    # 检查文件是否以.json结尾\n",
    "    if filename.endswith('.json'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        print(file_path)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:  \n",
    "            qa_data = json.load(file)\n",
    "        score_list=[] \n",
    "        all_bleu_avg=0\n",
    "        all_bleu1=0\n",
    "        all_bleu2=0\n",
    "        all_bleu3=0\n",
    "        all_bleu4=0\n",
    "        num=0\n",
    "        for qa in tqdm(qa_data):\n",
    "            query = qa['answer']\n",
    "            llm_ans = qa['llm_ans']  \n",
    "\n",
    "            rougeL=rougeL_score(llm_ans,query)\n",
    "            bleu_avg, bleu1, bleu2, bleu3, bleu4=bleu_score(llm_ans,query,True)\n",
    "            all_bleu_avg+=bleu_avg\n",
    "            all_bleu1+=bleu1\n",
    "            all_bleu2+=bleu2\n",
    "            all_bleu3+=bleu3\n",
    "            all_bleu4+=bleu4\n",
    "            score_list.append(rougeL)\n",
    "            num+=1\n",
    "        print(filename,',avg_rougeL: ',sum(score_list)/len(score_list),flush=True)\n",
    "        print(filename,',avg_bleu_avg: ',all_bleu_avg/num,flush=True)\n",
    "        print(filename,',avg_bleu1: ',all_bleu1/num,flush=True)\n",
    "        print(filename,',avg_bleu2: ',all_bleu2/num,flush=True)\n",
    "        print(filename,',avg_bleu3: ',all_bleu3/num,flush=True)\n",
    "        print(filename,',avg_bleu4: ',all_bleu4/num,flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9482d312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import jieba\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 对文本进行分词并进行处理\n",
    "def preprocess_text(text):\n",
    "    return ' '.join(jieba.cut(text)).split()\n",
    "\n",
    "folder_path = 'dataforchunk/ccc_eval'\n",
    "for filename in os.listdir(folder_path):\n",
    "    # 检查文件是否以.json结尾\n",
    "    if filename.endswith('.json'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:  \n",
    "            qa_data = json.load(file)\n",
    "        score_list=[] \n",
    "        for qa in tqdm(qa_data):\n",
    "            query = qa['answer']\n",
    "            llm_ans = qa['llm_ans']  \n",
    "\n",
    "            processed_reference = preprocess_text(query)\n",
    "            processed_candidate = preprocess_text(llm_ans)\n",
    "\n",
    "            score = meteor_score([processed_reference], processed_candidate)\n",
    "            score_list.append(score)\n",
    "        print(filename,',avg_sim: ',sum(score_list)/len(score_list),flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c588578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('dataforchunk/aaa/OmniEval_ratio_sc_7B_oec.json', 'r', encoding='utf-8') as file:  \n",
    "    qa_data = json.load(file)\n",
    "all_outline={}\n",
    "all_core={}\n",
    "for item in qa_data:\n",
    "    all_outline['文档大纲：\\n'+'\\n'.join(item[\"outline\"])]=item[\"final_chunk\"]\n",
    "    all_core['文档关键信息：\\n'+'\\n'.join(item[\"core_content\"])]=item[\"final_chunk\"]\n",
    "with open('dataforchunk/aaa/a_OmniEval_ratio_sc_7B_oec.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump([all_outline,all_core], f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f7fdea76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('dataforchunk/ccc/OmniEval_ratio_sc_7B_oec.json', 'r', encoding='utf-8') as file:  \n",
    "    qa_data = json.load(file)\n",
    "all_core={}\n",
    "for item in qa_data:\n",
    "    for i,iit in enumerate(item[\"final_chunk\"]):\n",
    "        all_core['文本块关键信息：\\n'+item[\"outline\"][i].replace('\\n',' ').strip()+'\\n'+item[\"core_content\"][i].replace('\\n',' ').strip()]=item[\"final_chunk\"]+item[\"core_content\"]\n",
    "with open('dataforchunk/ccc/a_OmniEval_ratio_sc_7B_oec.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump([all_core], f, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
